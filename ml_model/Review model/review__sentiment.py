# -*- coding: utf-8 -*-
"""Review _sentiment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1s6ehBrMIbAxObiDoHjljDUnjZXjsqvSP
"""

import pandas as pd
train=pd.read_csv('/content/Restaurant_Reviews.tsv', delimiter = '\t', quoting = 3)
train

import pandas as pd
x=train['Review']
y=train['Liked']
import string 
punct=string.punctuation
from spacy.lang.en.stop_words import STOP_WORDS
stopwords=list(STOP_WORDS)

train_0=train[train['Liked']==0]
train_1=train[train['Liked']==1]

train_0=train_0.sample(len(train_1),random_state=42)
new=train_0.append(train_1)
x=new['Review']
y=new['Liked']

import en_core_web_sm
import re

nlp = en_core_web_sm.load()

# def cleaned_data(sentence):
#     doc=nlp(sentence)
#     lines=""
#     for tokens in doc:
#       if str(tokens).isalpha():
#         if str(tokens) not in stopwords and str(tokens) != "-pron-" and str(tokens) not in [punct,'...','-pron- ','@','^^^',',','   ', '.','#', '"', ':', ')', '(', '-', '!', '?', '|', ';', "'", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\', '•',  '~', '@', '£', 
# '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', 
# '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', 
# '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', 
# '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√']:

            
#             tokens =tokens.lemma_
#             tokens=str(tokens).replace('@',"")
#             tokens=str(tokens).replace('-pron-',"")
#             ttokens=str(tokens).replace('#','a')
#             tokens=str(tokens).replace('?',"")
#             tokens= re.sub(r'http\S+', '', tokens)
#             tokens= re.sub("@[\w]*", '', tokens)
#             tokens=tokens.lower()

#             lines=lines+" "+str(tokens)
#     return lines

# import pandas as pd
# train=pd.read_csv('/content/Restaurant_Reviews.tsv', delimiter = '\t', quoting = 3)
# train['Review'] = train['Review'].apply(lambda x:cleaned_data(x) )
# train

# train_0=train[train['Liked']==0]
# train_1=train[train['Liked']==1]
# train_0=train_0.sample(len(train_1),random_state=42)
# new=train_0.append(train_1)
# x=new['Review']
# y=new['Liked']
# train

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix,accuracy_score,f1_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import SGDClassifier,LogisticRegression
from sklearn.naive_bayes import GaussianNB

train_x,test_x,train_y,test_y=train_test_split(x,y,test_size=.2,random_state=42,stratify=y)
train_x

pipe_SGDClassifier=Pipeline([('tfid',TfidfVectorizer()),('rfc',SGDClassifier())])
pipe_SGDClassifier.fit(train_x,train_y)
pre=pipe_SGDClassifier.predict(test_x)
confusion_matrix(test_y,pre),accuracy_score(test_y,pre),f1_score(test_y,pre)

pipe.predict(["your website have good  feature but you worked hard so i am good rating  but your project is bad"])

pipe=Pipeline([('tfid',TfidfVectorizer()),('rfc',LogisticRegression(C=1000))])
pipe.fit(train_x,train_y)
pre=pipe.predict(test_x)
confusion_matrix(test_y,pre),accuracy_score(test_y,pre),f1_score(test_y,pre)

pipe=Pipeline([('tfid',TfidfVectorizer()),('rfc',RandomForestClassifier(n_estimators=400))])
pipe.fit(train_x,train_y)
pre=pipe.predict(test_x)
confusion_matrix(test_y,pre),accuracy_score(test_y,pre),f1_score(test_y,pre)

import pickle
# open a file, where you ant to store the data
file = open('SGDClassifier.pkl', 'wb')

# SGDClassifier giving highest accuracy among them f1_score .8

# dump information to that file
pickle.dump(pipe_SGDClassifier, file)

model=pickle.load(open('SGDClassifier.pkl','rb'))
# model.predict(["this is bad"])

